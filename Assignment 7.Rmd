---
title: "Assignment 7"
author: "Chamundeswari Koppisetti"
date: "10/24/2020"
output: pdf_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Exercise 5.3.1 Rejection sampling
#### Let $f$ and $g$ be two probability densities on $(0,\infty)$, such that
$$
\begin{aligned}
  f(x) \propto \sqrt{4+x}\,x^{\theta-1} e^{-x}, \quad
  g(x) \propto (2 x^{\theta-1} + x^{\theta-1/2}) e^{-x}, \quad x>0.
\end{aligned} 
$$
  

#### 5.3.1.1. Find the value of the normalizing constant for $g$, i.e., the constant $C$ such that
$$
\begin{aligned}
C\int_0^\infty (2 x^{\theta-1} +  x^{\theta-1/2}) e^{-x} \text{d} x=1.
\end{aligned} 
$$
#### Show that $g$ is a mixture of Gamma distributions.  Identify the component distributions and their weights in the mixture.
  
  
In order to calculate C, we need to evaluate the integral:
$$
\begin{aligned}
& \int_0^\infty (2 x^{\theta-1} +  x^{\theta-1/2}) e^{-x} \text{d} x \\
& = 2\int_0^\infty x^{\theta-1} e^{-x} \text{d} x + \int_0^\infty x^{\theta-1/2} e^{-x} \text{d} x \\
& = 2\Gamma(\theta) + \Gamma(\theta + 1/2) \\
\end{aligned}
$$
Hence, $C = {1 \over 2\Gamma(\theta) + \Gamma(\theta + 1/2)}$

and $g(x)$ is a mixture of $Gamma(\theta, 1)$ and $Gamma(\theta + 1/2, 1)$, with weights $2 C\Gamma(\theta)$ and $C\Gamma(\theta + {1\over2} )$ respectively. The derivation is shown below:
$$
\begin{aligned}
g(x) & =C (2 x^{\theta-1} + x^{\theta-1/2}) e^{-x} \\
& = C(2 x^{\theta-1} e^{-x} + x^{\theta-1/2} e^{-x}) \\
& = C(2 \Gamma(\theta) Gamma(\theta, 1) + \Gamma(\theta + {1\over2} )Gamma(\theta + {1\over2}, 1)) \\
& = 2 C\Gamma(\theta) Gamma(\theta, 1) + C\Gamma(\theta + {1\over2} )Gamma(\theta + {1\over2}, 1) 
\end{aligned}
$$
  

#### 5.3.1.2. Design a procedure (pseudo-code) to sample from $g$; implement it in an R function; draw a sample of size $n = 10,000$ using your function for at least one $\theta$ value; plot the kernel density estimation of $g$ from your sample and the true density in one figure.  

```{r}
# Pseudo-code:
# Set initial parameters
# For i in 1 : N
# choose one of the Gamma distributions according to the weights
# generate a random value from the chosen distribution and save the random values
# Return sample
```


Implement in R function:
Input the initial parameters, Set theta as 3 (can be any value 2.5, 4, etc.). Set x as $[0, 4\theta]$ with step equal to 0.001. Calculate the weights of each Gamma distribution and $\alpha$, set $\beta$ of each Gamma distribution as 1.
```{r}
theta <- 3
x <- seq(0, 4 * theta, 0.001)
weight1 <- 2 * gamma(theta)
weight2 <- gamma(theta + 1/2)
p <- c(weight1, weight2) / sum(weight1, weight2)
alpha <- c(theta, theta + 1/2)
beta <- c(1, 1) 
```

Implement the pseudo-code into an R function:
```{r}
mix_sample <- function(n, p, alpha, beta) {
  temp <- sample(length(p), n, replace = TRUE, prob = p)
  return(rgamma(n, alpha[temp], beta[temp]))
}
```

Use the function to generate a random sample from $g$:
```{r}
n <- 10000
sample <- mix_sample(n, p, alpha, beta)
head(sample, 50)
```

Function to return the values of $g$ according to $x$:
```{r}
dMixGamma <- function(x, p, alpha, beta) {
  n <- length(x)
  mix <- length(p)
  result <- rowSums(sapply(1:mix, 
                           function(i) p[i] * dgamma(x, alpha[i], beta[i])))
  return(result)
}
```

Finally, plot the estimation and true density:
```{r}
g_x <- dMixGamma(x, p, alpha, beta)
plot(x, g_x, col = "blue", type = "l", 
     ylab = "g(x)-Density", main = paste("Theta = ", theta))
hist(sample, freq = FALSE, add = TRUE)
```

We can set $\theta$ as a different value, the estimated density will match or equal to the true density as the sample size increases


#### 5.3.1.3. Design a procedure (pseudo-code) to use rejection sampling to sample from $f$ using $g$ as the instrumental distribution. Overlay the estimated kernel density of a random sample generated by your procedure and $f$.

We can set the pseudo-code and it's implementation as following:
Since $g(0) = q(0) = 0$, it will return error when we calculate ${0 \over 0}$. So, we need to drop them to calculate M.
```{r}
# Find M that satisfied q(x) < M * g(x)
# Repeat the following steps:
# generate sample X~g(x) and U~Unif(0, 1)
# save the sample values that satisfied U < q(X)/(M * g(X))
# if we get enough sample, break 
# Return sample
```
``d_qx`` return the results of $\sqrt{4+x}\,x^{\theta-1} e^{-x}$ according to the value of $X$,
```{r}
d_qx <- function(x) {
  return(sqrt(4 + x) * x^(theta - 1) * exp(-x))
}
q_x <- d_qx(x)
M <- max(q_x[-1] / g_x[-1])

rej_sample <- function(n, p, alpha, beta) {
  ndone <- 0
  result <- rep(NA, n)
  while (TRUE) {
    cand <- mix_sample(n, p, alpha, beta)
    ratio <- d_qx(cand) / (M * dMixGamma(cand, p, alpha, beta))
    u <- runif(n)
    accept <- u < ratio
    naccept <- min(n - ndone, sum(accept))
    result[(ndone + 1) : (ndone + naccept)] <- cand[accept][1 : naccept]
    ndone <- ndone + naccept
    if (ndone == n) break
  }
  return(result)
}
```

Check the value of M, we can see that $M * g(x) > q(x)$. The blue line is $M * g(x)$, the red line is $q(x) = \sqrt{4+x}\,x^{\theta-1} e^{-x}$
```{r}
plot(x, M * g_x, col = "blue", type = "l", ylab = "Density")
lines(x, q_x, col = "red")
```

Since $\int_0^\infty \sqrt{4+x}\,x^{\theta-1} e^{-x}\text{d} x \neq 1$ , transform probability density function $f(x)$ as following:
$$
\begin{aligned}
\int_0^\infty \sqrt{4+x}\,x^{\theta-1} e^{-x}\text{d} x = H \\
f(x) = {1 \over H} \sqrt{4+x}\,x^{\theta-1} e^{-x}
\end{aligned}
$$
In this case we set the step of $x$ as $0.001 = \Delta x$. We can use ${\sum q(x) \over 1000}$ as an approximation of $H$. We can see that, estimated density matches the true density.Sample size = 1500
```{r}
q_x2 <- sapply(seq(0, 10 * theta, 0.001), d_qx)
H <- sum(q_x2) / 1000
f_x <- q_x / H

x2 <- rej_sample(1500, p, alpha, beta)
plot(x, f_x, col = "red", type = "l", 
     ylab = "f(x)-Density", main = paste("Theta = ", theta))
hist(x2, freq = FALSE, add = TRUE)
```

Set the sample size as $10^6$
```{r}
x2 <- rej_sample(1e6, p, alpha, beta)
plot(x, f_x, col = "red", type = "l", 
     ylab = "f(x)-Density", main = paste("Theta = ", theta))
hist(x2, 150, freq = FALSE, add = TRUE)
```

### Exercise 6.3.1 Normal mixture revisited
#### Consider again the normal mixture example, except that the parameters of the normal distributions are considered unknown. Suppose that prior for $\mu_1$ and $\mu_2$ are $N(0, 10^2)$, that the prior for $1/\sigma_1^2$ and $1/\sigma_2^2$ are $\Gamma(a, b)$ with shape $a = .5$ and scale $b = 10$. Further, all the priors are independent. Design an MCMC using the Gibbs sampling approach to estimate all 5 parameters. You may use the `arms()`function in package **HI**. Run your chain for sufficiently long and drop the burn-in period. Plot the histogram of the results for all the parameters.

In order to use Gibbs sampling to estimate all 5 parameters, we need to derive the posterior density of each parameter.
Because all parameters are independent, and we can assume the prior distributions on $\mu_1$ and $\mu_2$ are $N(0, 10^2)$, prior for $1/\sigma_1^2$ and $1/\sigma_2^2$ are $\Gamma(a, b)$ with shape $a = 0.5$ and scale $b = 10$. Further, we assume $\delta$ is $U(0, 1)$. So the posterior density is:
$$
q(\delta, \mu_1, \mu_2, \sigma_1, \sigma_2| X) \propto 
q(X | \delta, \mu_1, \mu_2, \sigma_1, \sigma_2) * 
q(\delta) * q(\mu_1) * q(\mu_2) * q(\delta_1)* q(\delta_2)
$$
We can define the Gibbs sampling approach.Prepare the initial data:
```{r}
delta <- 0.7
n <- 100
set.seed(123)
u <- rbinom(n, prob = delta, size = 1)
x <- rnorm(n, ifelse(u == 1, 7, 10), 0.5)
```
Loglikelihood function:
$\theta$ contains all the parameters, $\theta = [\delta,\mu_1, \mu_2, \sigma_1, \sigma_2]$

```{r, error = F}
logpost <- function(theta, x, idx) {
  loglike <<- sum(log((theta[1] * dnorm(x, theta[2], theta[4]) +
                       (1 - theta[1]) * dnorm(x, theta[3], theta[5])))) +
    log(dnorm(theta[2], 0, 10)) + log(dnorm(theta[3], 0, 10)) +
    log(dgamma((theta[4])^(-2), shape = .5, scale = 10)) + 
    log(dgamma((theta[5])^(-2), shape = .5, scale = 10))
  return(loglike)
}
```
Next, MCMC based on the Gibbs sampler uses the ARMS algorithm from R package HI. 
Set different range for $\delta$, $\mu$ and $\sigma$:

```{r}
mcmc <- function(niter, thetaInit, x, nburn = 0.3 * niter) {
  p <- length(thetaInit)
  thetaCurrent <- thetaInit
  out <- matrix(thetaInit, niter, p, byrow = TRUE)
  logFC <- function(th, idx) {
    theta <- thetaCurrent
    theta[idx] <- th
    logpost(theta, x, idx)
  }
  ## Gibbs sampling
  for (i in 2:niter) {
    for (j in 1:p) {
      if (j == 1) {
        out[i, j] <- thetaCurrent[j] <-
          HI::arms(thetaCurrent[j], logFC,
                   function(x, idx = j) ((x > 0) * (x < 1)), 1, idx = j)
      }
      if (j == 2 | j == 3) {
        out[i, j] <- thetaCurrent[j] <-
          HI::arms(thetaCurrent[j], logFC,
                   function(x, idx) ((x > -30) * (x < 30)), 1, idx = j)
      }
      if (j == 4 | j == 5) {
        out[i, j] <- thetaCurrent[j] <-
          HI::arms(thetaCurrent[j], logFC,
                   function(x, idx) (x > 0.2) * (x < 10), 1, idx = j)
      }
    }
  }
  out[-(1:nburn), ]
}
```
Now, set the initial parameters, and run the MCMC.

```{r}
thetaInit <- c(delta = 0.5, u1 = 5, u2 = 5, sig1 = 5, sig2 = 5)
niter <- 5000
sim <- mcmc(niter, thetaInit, x, nburn = 2000)
```

```{r}
par(mfrow = c(1, 2))
plot(ts(sim[, 2]), ylab = 'Mu_1', xlab = "Time");
plot(ts(sim[, 3]), ylab = 'Mu_2', xlab = "Time");
plot(ts(sim[, 4]), ylab = 'Sigma_1', xlab = "Time");
plot(ts(sim[, 5]), ylab = 'Sigma_2', xlab = "Time");
plot(ts(sim[, 1]), ylab = 'Delta', xlab = "Time")
```

The estimations of the algorithm are pretty close to the exact values 
$$
\begin{aligned}
\theta & = [ \delta = 0.7, \mu_1 = 7, \mu_2 = 10, \sigma_1 = 0.5, \sigma_2 = 0.5 ] \\
or \\
\theta &= [ \delta = 0.3, \mu_1 = 10, \mu_2 = 7, \sigma_1 = 0.5, \sigma_2 = 0.5 ]
\end{aligned}
$$






